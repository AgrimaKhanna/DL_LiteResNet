{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Total number of params 4697742\n",
      "Total layers 28\n",
      "The Total number of parameters in the model are:  4697742\n",
      "\n",
      "Epoch: 0\n",
      "Batch_idx: 0 | Train Loss: 2.434 | Train Acc: 11.000% (11/100)\n",
      "Batch_idx: 0 | Test Loss: 2.304 | Test Acc: 3.125% (1/32)\n",
      "Batch_idx: 1 | Test Loss: 2.303 | Test Acc: 4.688% (3/64)\n",
      "Batch_idx: 2 | Test Loss: 2.302 | Test Acc: 8.333% (8/96)\n",
      "Batch_idx: 3 | Test Loss: 2.304 | Test Acc: 9.000% (9/100)\n",
      "Saving..\n",
      "\n",
      "Epoch: 1\n",
      "Batch_idx: 0 | Train Loss: 2.366 | Train Acc: 11.000% (11/100)\n",
      "Batch_idx: 0 | Test Loss: 2.312 | Test Acc: 3.125% (1/32)\n",
      "Batch_idx: 1 | Test Loss: 2.309 | Test Acc: 4.688% (3/64)\n",
      "Batch_idx: 2 | Test Loss: 2.304 | Test Acc: 9.375% (9/96)\n",
      "Batch_idx: 3 | Test Loss: 2.297 | Test Acc: 10.000% (10/100)\n",
      "Saving..\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "write() argument must be str, not dict",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 153\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;66;03m# Saving to a text file\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../results/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/results_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m--> 153\u001b[0m     \u001b[43mfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m result_logs\u001b[38;5;241m.\u001b[39mclose()  \n",
      "\u001b[0;31mTypeError\u001b[0m: write() argument must be str, not dict"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn  \n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import argparse\n",
    "import yaml\n",
    "import math\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import utils\n",
    "# from ResNetLite import ResNetLite \n",
    "from lookahead import Lookahead \n",
    "import config\n",
    "from collections import defaultdict\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Subset\n",
    "# Function to select 100 images per class\n",
    "def select_n_per_class(dataset, n=100):\n",
    "    class_counts = defaultdict(int)\n",
    "    indices = []\n",
    "\n",
    "    for idx in range(len(dataset)):\n",
    "        _, label = dataset[idx]\n",
    "        if class_counts[label] < n:\n",
    "            indices.append(idx)\n",
    "            class_counts[label] += 1\n",
    "            if all(count == n for count in class_counts.values()):\n",
    "                break\n",
    "    return indices\n",
    "\n",
    "if __name__ == '__main__': \n",
    "    # Setting the arguments\n",
    "    # parser = argparse.ArgumentParser(description='PyTorch CIFAR10 Training')\n",
    "    # parser.add_argument('--model', default='LiteResNet', type=str, help='name of resnet architecture from config') \n",
    "    # args = parser.parse_args()\n",
    "    model = 'LiteResNet'\n",
    "\n",
    "    hyperparams = config.hyperparams[model]\n",
    "    # Setting the device\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    # Setting the the best accuracy to 0 and start epoch to 0\n",
    "    best_acc = 0  # best test accuracy\n",
    "    start_epoch = 0 # start from epoch 0\n",
    "    # Setting the experiment name\n",
    "    exp = model\n",
    "    # Preparing the dataset\n",
    "    train_transformations = [transforms.ToTensor()]\n",
    "    test_transformations = [transforms.ToTensor()]\n",
    "    # Adding the transforms based on the hyperparameters\n",
    "    if hyperparams[\"data_augmentation\"]: \n",
    "        train_transformations.append(transforms.RandomCrop(32, padding=4)) \n",
    "        train_transformations.append(transforms.RandomHorizontalFlip()) \n",
    "    if hyperparams[\"data_normalize\"]: \n",
    "        train_transformations.append(transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))) \n",
    "        test_transformations.append(transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))) \n",
    "    # Adding the transforms to the dataset - will be used for both train and test\n",
    "    train_transformations = transforms.Compose(train_transformations) \n",
    "    test_transformations = transforms.Compose(test_transformations) \n",
    "    # Getting the train set and loading it\n",
    "    train_dataset = torchvision.datasets.CIFAR10(root='../data_10', train=True, download=True, transform=train_transformations)\n",
    "    \n",
    "    # Using the function to filter indices\n",
    "    filtered_indices = select_n_per_class(train_dataset, 10)\n",
    "    subset_dataset = Subset(train_dataset, filtered_indices)\n",
    "    train_dataset = subset_dataset \n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=hyperparams[\"batch_size\"], shuffle=True, num_workers=hyperparams[\"num_workers\"])\n",
    "    # Getting the test set and loading it\n",
    "\n",
    "    test_dataset = torchvision.datasets.CIFAR10(root='../data_10', train=False, download=True, transform=test_transformations)\n",
    "    # Using the function to filter indices\n",
    "    filtered_indices = select_n_per_class(test_dataset, 10)\n",
    "    subset_dataset = Subset(test_dataset, filtered_indices)\n",
    "    test_dataset = subset_dataset \n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=int(hyperparams[\"batch_size\"]/4), shuffle=False, num_workers=hyperparams[\"num_workers\"])\n",
    "    \n",
    "    # Creating the ResNet model and getting total number of parameters\n",
    "    resnet_model, total_params = utils.ResNetLite(config=hyperparams) \n",
    "    hyperparams['total_params'] = total_params \n",
    "    print('The Total number of parameters in the model are: ', total_params) \n",
    "    # Moving the model to the device\n",
    "    resnet_model = resnet_model.to(device)\n",
    "    # Getting cuda if available\n",
    "    if device == 'cuda':\n",
    "        resnet_model = torch.nn.DataParallel(resnet_model)\n",
    "        cudnn.benchmark = True\n",
    "    \n",
    "    if (\"weights_init_type\" in hyperparams): \n",
    "        def init_weights(m, type='default'): \n",
    "            if (isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d)) and hasattr(m, 'weight'): \n",
    "                if type == 'xavier_uniform_': torch.nn.init.xavier_uniform_(m.weight.data)\n",
    "                elif type == 'normal_': torch.nn.init.normal_(m.weight.data, mean=0, std=0.02)\n",
    "                elif type == 'xavier_normal': torch.nn.init.xavier_normal(m.weight.data, gain=math.sqrt(2))\n",
    "                elif type == 'kaiming_normal': torch.nn.init.kaiming_normal(m.weight.data, a=0, mode='fan_in')\n",
    "                elif type == 'orthogonal': torch.nn.init.orthogonal(m.weight.data, gain=math.sqrt(2))\n",
    "                elif type == 'default': pass \n",
    "        resnet_model.apply(lambda m: init_weights(m=m, type=hyperparams[\"weights_init_type\"])) \n",
    "\n",
    "    if hyperparams[\"resume_ckpt\"]:        \n",
    "        # Load checkpoint.\n",
    "        print('Getting the last checkpoint and training')\n",
    "        checkpoint = torch.load(hyperparams[\"resume_ckpt\"])\n",
    "        resnet_model.load_state_dict(checkpoint['net'])\n",
    "        best_acc = checkpoint['acc']\n",
    "        start_epoch = checkpoint['epoch']\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Setting the optimizer\n",
    "    optimizer = utils.get_optimizer(resnet_model, hyperparams['optimizer'], hyperparams['lr'], hyperparams['momentum'], hyperparams['weight_decay'])\n",
    "\n",
    "    # Setting the lookahead optimizer\n",
    "    if (\"lookahead\" in hyperparams) and hyperparams[\"lookahead\"]: optimizer = Lookahead(optimizer, k=5, alpha=0.5) # Initialize Lookahead \n",
    "\n",
    "    # Setting the scheduler\n",
    "    scheduler = utils.get_scheduler(hyperparams['lr_sched'],optimizer)\n",
    "\n",
    "    # Setting the writer to write the results\n",
    "    result_logs = SummaryWriter('../results/'+exp) \n",
    " \n",
    "    history = {\n",
    "        'epoch': [],\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'test_loss': [],\n",
    "        'test_acc': [],\n",
    "        'best_acc': []\n",
    "    }\n",
    "\n",
    "    # Training and testing for each epoch and saving the best model (based on test accuracy)\n",
    "    # for epoch in range(start_epoch, hyperparams[\"max_epochs\"]): \n",
    "    for epoch in range(start_epoch, 2): \n",
    "        # Training the model\n",
    "        resnet_model, optimizer, train_loss, train_acc = utils.train(resnet_model, train_loader, epoch, hyperparams, optimizer, criterion, result_logs, device)\n",
    "        # Testing the model\n",
    "        resnet_model, optimizer, best_acc, test_loss, test_acc = utils.test(resnet_model, test_loader, epoch, hyperparams, optimizer, criterion, result_logs, exp, best_acc ,device)\n",
    "        scheduler.step()\n",
    "\n",
    "        history['epoch'].append(epoch)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['test_loss'].append(test_loss)\n",
    "        history['test_acc'].append(test_acc)\n",
    "        history['best_acc'].append(best_acc)\n",
    "    \n",
    "    # Saving to a text file\n",
    "    with open(f\"../results/{exp}/results_{exp}.txt\", \"w\") as file:\n",
    "        file.write(history)\n",
    "    result_logs.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving to a text file\n",
    "with open(f\"../results/{exp}/results_{exp}.txt\", \"w\") as file:\n",
    "    file.write(str(history))\n",
    "result_logs.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
